{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-12T18:25:21.172039Z",
     "start_time": "2025-02-12T18:25:21.169041Z"
    }
   },
   "outputs": [],
   "source": [
    "#python3 -m venv cenv\n",
    "# source env/bin/activate\n",
    "# source ~/.zshrc"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-12T23:58:26.062383Z",
     "start_time": "2025-02-12T23:58:24.848318Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "DATA_DIR = \"data\"\n",
    "\n",
    "def load_documents(directory):\n",
    "    loader = DirectoryLoader(directory, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    return loader.load()\n",
    "\n",
    "def chunk_documents(documents, chunk_size=1000, overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "def get_embeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    return HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "def store_chroma(docs, embeddings, persist_dir=\"./chroma_db\"):\n",
    "    vectorstore = Chroma.from_documents(docs, embeddings, persist_directory=persist_dir)\n",
    "    return vectorstore\n",
    "\n",
    "def load_chroma(persist_dir=\"./chroma_db\", embeddings=None):\n",
    "    return Chroma(persist_directory=persist_dir, embedding_function=embeddings)\n",
    "\n",
    "def query_chroma(query, vectorstore):\n",
    "    docs = vectorstore.similarity_search(query, k=3)\n",
    "    return [(doc.metadata, doc.page_content) for doc in docs]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-12T23:58:26.284598Z",
     "start_time": "2025-02-12T23:58:26.278851Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T23:55:16.931565Z",
     "start_time": "2025-02-12T23:55:16.927249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# rm -rf ./chroma_db\n",
    "# lsof | grep chroma_db"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "# Load and process data\n",
    "documents = load_documents(DATA_DIR)\n",
    "chunks = chunk_documents(documents)\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = get_embeddings()\n",
    "\n",
    "# Store in ChromaDB\n",
    "vectorstore = store_chroma(chunks, embeddings)\n",
    "\n",
    "# Load ChromaDB for querying\n",
    "vectorstore = load_chroma(embeddings=embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-12T23:59:04.427676Z",
     "start_time": "2025-02-12T23:58:30.563820Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8g/z0__2mts0rnbjh3kq8zsfz2w0000gn/T/ipykernel_68984/3514957188.py:25: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  return Chroma(persist_directory=persist_dir, embedding_function=embeddings)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T23:59:15.677678Z",
     "start_time": "2025-02-12T23:59:15.653988Z"
    }
   },
   "cell_type": "code",
   "source": "display(vectorstore)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x11a924a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T23:59:17.838313Z",
     "start_time": "2025-02-12T23:59:17.022865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"what is Pinus greggii\"\n",
    "results = query_chroma(query, vectorstore)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T23:59:18.909037Z",
     "start_time": "2025-02-12T23:59:18.903948Z"
    }
   },
   "cell_type": "code",
   "source": "results",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'author': 'umax',\n",
       "   'creationdate': 'D:20001016124829',\n",
       "   'creator': 'Adobe PageMaker 6.0',\n",
       "   'keywords': '',\n",
       "   'moddate': 'D:20001019130050',\n",
       "   'page': 88,\n",
       "   'page_label': '89',\n",
       "   'producer': 'Acrobat Distiller 2.1 for Power Macintosh',\n",
       "   'source': 'data/Camcore_BookAllChapters.pdf',\n",
       "   'subject': '',\n",
       "   'title': 'Final camcore book pt 1',\n",
       "   'total_pages': 250},\n",
       "  'PINUS GREGGII\\n73'),\n",
       " ({'author': 'umax',\n",
       "   'creationdate': 'D:20001016124829',\n",
       "   'creator': 'Adobe PageMaker 6.0',\n",
       "   'keywords': '',\n",
       "   'moddate': 'D:20001019130050',\n",
       "   'page': 88,\n",
       "   'page_label': '89',\n",
       "   'producer': 'Acrobat Distiller 2.1 for Power Macintosh',\n",
       "   'source': 'data/Camcore_BookAllChapters.pdf',\n",
       "   'subject': '',\n",
       "   'title': 'Final camcore book pt 1',\n",
       "   'total_pages': 250},\n",
       "  'PINUS GREGGII\\n73'),\n",
       " ({'author': 'umax',\n",
       "   'creationdate': 'D:20001016124829',\n",
       "   'creator': 'Adobe PageMaker 6.0',\n",
       "   'keywords': '',\n",
       "   'moddate': 'D:20001019130050',\n",
       "   'page': 68,\n",
       "   'page_label': '69',\n",
       "   'producer': 'Acrobat Distiller 2.1 for Power Macintosh',\n",
       "   'source': 'data/Camcore_BookAllChapters.pdf',\n",
       "   'subject': '',\n",
       "   'title': 'Final camcore book pt 1',\n",
       "   'total_pages': 250},\n",
       "  'PINUS GREGGII\\n53\\nPINUS GREGGII\\nTREE DESCRIPTION\\nIn the early 1990s, the varying growth and morphology of\\nPinus greggii trees in CAMCORE field trials led foresters to\\nbegin recognizing that the species’ northern and southern\\npopulations in Mexico represented distinct taxa.\\nSubsequent taxonomic and monoterpene studies by\\nDonahue and López-Upton (1996) and Donahue et al.\\n(1996) culminated in the classification of P. greggii as two\\nvarieties.  Trees in northern populations from Nuevo León\\nand Coahuila, Mexico are known as P . greggii Engelm. ex\\nParl. var. greggii, and those 360 km to the south in the states\\nof Hidalgo, Puebla, Querétaro, San Luis Potosí and Veracruz\\nare known as P. greggii Engelm. ex Parl. var. australis\\nDonahue & López-Upton (Donahue and López-Upton\\n1999).\\nPinus greggii var . greggii\\nPinus greggii var. greggii is a small tree that is typically 6 to\\n15 m in height with dbh of 22 to 40 cm at maturity.\\nMature trees have a rounded and sometimes irregular')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T02:20:09.417834Z",
     "start_time": "2025-02-13T02:20:09.335927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from langchain.prompts import PromptTemplate\n",
    "#\n",
    "# # Define the Prompt Template\n",
    "# prompt = PromptTemplate(\n",
    "#     input_variables=[\"context\", \"question\"],\n",
    "#     template=\"Use the following context to answer the question:\\n\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "# )"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T16:46:05.211913Z",
     "start_time": "2025-02-13T16:46:04.173999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "LLM_MODEL = \"deepseek-llm\"\n",
    "\n",
    "def get_llm(model_name=LLM_MODEL):\n",
    "    return OllamaLLM(model=model_name)\n",
    "\n",
    "# RAG Pipeline Function\n",
    "def rag_pipeline(query, vectorstore):\n",
    "    # embeddings = get_embeddings()\n",
    "    # vectorstore = load_chroma(embeddings=embeddings)\n",
    "\n",
    "    retrieved_docs = query_chroma(query, vectorstore)\n",
    "    context = \"\\n\".join([doc[1] for doc in retrieved_docs])\n",
    "\n",
    "    llm = get_llm()\n",
    "    prompt = f\"Answer the question based on the following context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return response\n",
    "\n",
    "#grid search - chunk size - hyperparam"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ollama'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain_ollama\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m OllamaLLM\n\u001B[1;32m      3\u001B[0m LLM_MODEL \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdeepseek-llm\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mget_llm\u001B[39m(model_name\u001B[38;5;241m=\u001B[39mLLM_MODEL):\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_ollama/__init__.py:9\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"This is the langchain_ollama package.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03mIt provides infrastructure for interacting with the Ollama service.\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mimportlib\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m metadata\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain_ollama\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchat_models\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ChatOllama\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain_ollama\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01membeddings\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m OllamaEmbeddings\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain_ollama\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllms\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m OllamaLLM\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_ollama/chat_models.py:54\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunction_calling\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m convert_to_openai_tool\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpydantic\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m TypeBaseModel, is_basemodel_subclass\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mollama\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AsyncClient, Client, Message, Options\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpydantic\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m BaseModel, PrivateAttr, model_validator\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpydantic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjson_schema\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m JsonSchemaValue\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'ollama'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T16:46:05.259922Z",
     "start_time": "2025-02-13T03:30:14.953258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"What is Pinus greggii?\"\n",
    "response = rag_pipeline(query, vectorstore)\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10577ee40>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
